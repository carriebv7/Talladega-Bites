---
title: "combined model"
author: "Talladega Bites"
date: "2025-04-02"
output: html_document
---

#Talladega Bites

```{r}
#remotes::install_github("eco4cast/neon4cast")
library(tidyverse)
library(neon4cast)
library(lubridate)
library(rMR)
library(arrow)
library(rjags)
require(ggplot2)
library(dplyr)
library(tidyr)
```

# Tick Data from Carrie
```{r}
dat <- read.csv("monthly_tick_temp.csv")
library(rjags)
```
The data is the tick and temperature data used in the other models. We only used data if we had the month/year available for both tick count and average monthly temperature. If there were two tick counts for a given month/year, we averaged them together so that we had an average for the month.

Formatting data:

```{r}
t = dat[1:58,1] #time
tick = round(dat[1:58,3]) #monthly average ticks
temp = dat[1:58,2] #monthly average temperature
data <- list(tick = tick, temp = temp, n = length(t))

data$temp <- round(data$temp) 
```


```{r}
plot(temp,tick)
```


# Quick Model Summary

For our combined internal and external factors model, we estimate mean tick count at time *i* (X[i]) using a carrying capacity model (density dependence as an internal factor) where **K** (carrying capacity) and **r** (growth rate) are dependent on **temperature** (our chosen external factor, for a start) and the state equation includes a term for **process error** (E). 

$$
X_i = r_i \cdot X_{i-1} \cdot \left(1 - \frac{X_{i-1}}{K_i} \right) + E_i
$$
$$
r_i = \beta_0 + \beta_1 \cdot \text{temp}
$$

$$
K_i = \exp(\beta_0 + \beta_1 \cdot \text{temp})
$$
$$
E_i \sim \text{Normal}(0, \tau)
$$
$$
\tau \sim \text{Gamma}(0.1, 0.1)
$$

To fit the model to historical data, we need months where we have both a tick count and a mean temperature, which is why you see me dropping rows missing one or the other in the code blocks above. Below, I define the model in JAGS-speak. Note that process error (E), r, and K are defined for each model loop. B0, B1 are the temperature effects on the rate variable, and B2, B3 are the temperature effects on the K, which shouldn't be changing, so they are not defined in each loop. We also have priors on the initial conditions at X[1], since at the first time point, we aren't sure what the ticks or temp at t-1 were.

r and K are linear relationships for now, but that may have to be adjusted. I have also wrapped K in an exponential function to avoid any negative values. I initially just put a max(0.001) term on it, but that introduced issues with the traceplots (the chains were getting hung at 0.001); using exp() gives the distribution smoother approach to 0 and allows the MCMC to keep running more smoothly.

-- look at correlation plots. Look for weird T shape (potato good, hugging is bad).
-- constrain parts of model. Set K as hard 300, can help 
-- run for 150 iters
-- can try running without temp dependence (temp seems to not be influencing much)



```{r}
# Convert 'month.year' to a Date object (assuming format is YYYY-MM)
dat$Date <- as.Date(paste0(dat$month.year, "-01"))

# Calculate time step differences in months
dat$TimeStep <- c(NA, diff(as.numeric(format(dat$Date, "%Y")) * 12 + as.numeric(format(dat$Date, "%m"))))

dat <- dat[1:58,]
dat$tick <- round(dat$tick)

# View result
head(dat)

```

```{r}
log_solution_model <- "
model{

  ## priors
  ## tau (time between steps) from the dataframe
  K ~ dnorm(300,0.01)
  r ~ dnorm(0,0.01)
  sigma ~ dgamma(0.1,0.1) ## sigma is E's precision for one month
  
  N[1] ~ dnorm(N_ic, tau_N_ic)  # Latent state initial value
  N_ic ~ dnorm(0, 0.1)        # Prior for initial state
  tau_N_ic ~ dgamma(0.1, 0.1)   # Precision on initial state
  
  ## process model
    for(i in 2:Ni){
      #tau_E[i] <- sigma*sqrt(2*tau[i])
      tau_E[i] <- sigma^tau[i]   # sigma is multiplicative for multiple months
      E[i] ~ dnorm(0, tau_E[i])
      N[i] <- max(0,((K*N[i-1]) / (N[i-1] + (K-N[i-1]))*exp(-r*tau[i])) + E[i])
    }
  
  ## data model
    for(i in 1:Ni){
      y[i] ~ dpois(max(0.001, N[i]))  # Ensures positive values only
    }
}
"
```

```{r}
data <- list(y=dat$tick, Ni=length(dat$tick),      ## tick data
             tau=dat$TimeStep
             )

j.model   <- jags.model (file = textConnection(log_solution_model),
                             data = data,
                             n.chains = 5)
```

And here we have the actual model running step. You can see that I have a lot of iterations and a burn-in period to help with convergence (which we're still not achieving with most, if not all, of the variables).

```{r}
out_1 <- coda.samples(model = j.model,
                      variable.names = c("r", "E", "N", "K", "sigma"),
                      n.iter = 150000,
                      burnin = 10000)

```


```{r}
plot(out_1[, c("r", "K", "sigma")])
```

```{r}
plot(out_1[, c("N[5]", "N[10]", "N[15]")])
```
```{r}
plot(out_1[, c("E[5]", "E[10]", "E[15]")])
```


```{r}
gelman.plot(out_1[, c("E[5]", "E[10]", "E[15]")])
```

```{r}
gelman.plot(out_1[, c("r", "K", "sigma")])
```




```{r}
gelman.diag(out_1[, c("r", "sigma", "K")])
```
```{r}
gelman.diag(out_1[, c("N[5]", "N[10]", "N[15]")])
```


```{r}
gelman.diag(out_1[, c("E[5]", "E[10]", "E[15]")])
```

Above, confirming what the traceplots and BGR plots (and my heart) told us, none of the BGR metrics are below 1.1, which tells us the variables have not converged. :( Except for our superstar E (process error) term! It's nice to have a positive control for what a converged variable might look like, if nothing else.

Let's look at some summary statistics:
```{r}
summary(out_1[, c("N[5]", "N[10]", "N[15]",
              "r", "K", "sigma",
              "E[5]", "E[10]", "E[15]")])
```

```{r}

combined_mcmc <- as.mcmc(do.call(rbind, out_1))

# convert to data frame
params_df <- as.data.frame(combined_mcmc)

params_subset <- params_df[, c("N[5]", "N[10]", "N[15]",
              "r", "K", "sigma",
              "E[5]", "E[10]", "E[15]")]
head(params_subset)
```
```{r}
pairs(params_subset, pch = 1, cex = 0.3)
```

# Time Series

```{r}
# Flatten MCMC output
out_matrix <- as.matrix(out_1)

# Time vector
time <- 1:length(dat$tick)

# Extract latent state samples
X_samples <- out_matrix[, grep("^N\\[", colnames(out_matrix))]

# Compute posterior summaries
X_median <- apply(X_samples, 2, median)
X_CI <- apply(X_samples, 2, quantile, probs = c(0.025, 0.975))

```

```{r}

# Base plot
plot(dat$Date, X_median, type = 'l', lwd = 2, col = "blue", ylim = c(-100, max(X_CI[2,]) * 1.1),
     ylab = "N", xlab = "Date")

# 95% Credible interval as blue ribbon
polygon(c(dat$Date, rev(dat$Date)),
        c(X_CI[1,], rev(X_CI[2,])),
        col = rgb(0, 0, 1, 0.2), border = NA)

# Add median line again on top of ribbon
lines(dat$Date, X_median, col = "blue", lwd = 2)

# Observed data points
points(dat$Date, data$y, pch = 21, bg = "white")

legend("topright",
       legend = c("Median latent state", "Observed counts", "95% Credible Interval"),
       col = c("blue", "black", NA),
       lwd = c(2, NA, NA),
       pch = c(NA, 21, NA),
       pt.bg = c(NA, "white", NA),
       fill = c(NA, NA, rgb(0, 0, 1, 0.2)),  # Add fill for CI
       border = c(NA, NA, NA),              # No border for fill
       bty = "n",
       cex = 0.8)


```


# add forecast interval to JAGS model

```{r}
N_forecast <- 12
Ni_obs <- length(dat$tick)

# Extend y with 12 NAs
y_full <- c(dat$tick, rep(NA, N_forecast))

# Assume constant monthly step from last timestep
# If TimeStep is in months (e.g., 1, 2, 3...), this just extends linearly
last_tau <- tail(dat$TimeStep, 1)
tau_forecast <- rep(1, N_forecast)  # each new step is 1 month
tau_full <- c(dat$TimeStep, tau_forecast)

# Total time points
Ni <- length(y_full)

data_forecast <- list(
  y = y_full,
  tau = tau_full,
  Ni = Ni
)

```

```{r}
j.model <- jags.model(
  file = textConnection(log_solution_model),
  data = data_forecast,
  n.chains = 5
)


out_forecast <- coda.samples(
  model = j.model,
  variable.names = c("r", "E", "N", "K", "sigma"),
  n.iter = 150000,
  burnin = 10000
)


```


```{r}
# Convert coda output to matrix
out_matrix <- as.matrix(out_forecast)

# Extract columns corresponding to N[...]
N_cols <- grep("^N\\[", colnames(out_matrix))
N_samples <- out_matrix[, N_cols]  # Each column is N[1], N[2], ..., N[Ni + 12]

# Confirm dimension
dim(N_samples)  # should be (n.iter * n.chains) rows Ã— (Ni + 12) columns
```

```{r}
# Compute posterior median and 95% CI for each time point
N_median <- apply(N_samples, 2, median)
N_CI <- apply(N_samples, 2, quantile, probs = c(0.025, 0.975))

```

```{r}
Ni_obs <- length(dat$Date)
Ni_total <- ncol(N_samples)
Ni_forecast <- Ni_total - Ni_obs

# Extend date vector 12 months forward
future_dates <- seq(from = max(dat$Date) + 1, by = "month", length.out = Ni_forecast)
all_dates <- c(dat$Date, future_dates)

# Split CI and median into observed + forecast parts
X_median_obs <- N_median[1:Ni_obs]
X_CI_obs <- N_CI[, 1:Ni_obs]

X_median_forecast <- N_median[(Ni_obs + 1):Ni_total]
X_CI_forecast <- N_CI[, (Ni_obs + 1):Ni_total]

```


```{r}
# Base plot
plot(all_dates, N_median, type = 'n', ylim = c(-50, max(N_CI[2,]) * 1.1),
     ylab = "N", xlab = "Date")

# 95% CI for observed
polygon(c(dat$Date, rev(dat$Date)),
        c(X_CI_obs[1,], rev(X_CI_obs[2,])),
        col = rgb(0, 0, 1, 0.2), border = NA)

# 95% CI for forecast
polygon(c(future_dates, rev(future_dates)),
        c(X_CI_forecast[1,], rev(X_CI_forecast[2,])),
        col = rgb(1, 0, 0, 0.2), border = NA)

# Median lines
lines(dat$Date, X_median_obs, col = "blue", lwd = 2)
lines(future_dates, X_median_forecast, col = "red", lwd = 2, lty = 2)

# Observed data points
points(dat$Date, data$y, pch = 21, bg = "white")

# Forecasted points
points(future_dates, X_median_forecast, pch = 4, bg = "red", col = "red")

# Legend
legend("topleft",
       legend = c("Observed Median", "Forecast Median", "Observed Data", "95% CI (Observed)", "95% CI (Forecast)"),
       col = c("blue", "red", "black", NA, NA),
       lwd = c(2, 2, NA, NA, NA),
       lty = c(1, 2, NA, NA, NA),
       pch = c(NA, NA, 21, NA, NA),
       pt.bg = c(NA, NA, "white", NA, "red"),
       fill = c(NA, NA, NA, rgb(0, 0, 1, 0.2), rgb(1, 0, 0, 0.2)),
       border = c(NA, NA, NA, NA, NA),
       bty = "n",
       cex = 0.8)

```

Now, we have to partition our uncertainty. The median line above is our deterministic prediction. We will zoom in on 2023 and onward in the plot to better see our uncertainty, and take out the confidence interval for the forecasting period from before. 

```{r}
dat$Date <- as.Date(paste0(dat$month.year, "-01"))
# Define cutoff date
cutoff_date <- as.Date("2023-01-01")

# Filter indices for zooming
zoom_idx_all <- which(all_dates >= cutoff_date)
zoom_idx_obs <- which(dat$Date >= cutoff_date)
zoom_idx_forecast <- which(future_dates >= cutoff_date)

zoomed_plot <- function() {
  # Base plot (zoomed)
  plot(all_dates[zoom_idx_all], N_median[zoom_idx_all], type = 'n',
       ylim = c(-50, max(N_CI[2, zoom_idx_all]) * 1.1),
       ylab = "N", xlab = "Date")
  
  # 95% CI for observed
  polygon(c(dat$Date[zoom_idx_obs], rev(dat$Date[zoom_idx_obs])),
          c(X_CI_obs[1, zoom_idx_obs], rev(X_CI_obs[2, zoom_idx_obs])),
          col = rgb(0, 0, 1, 0.2), border = NA)

  # Median lines
  lines(dat$Date[zoom_idx_obs], X_median_obs[zoom_idx_obs], col = "blue", lwd = 2)
  lines(future_dates[zoom_idx_forecast], X_median_forecast[zoom_idx_forecast], col = "red", lwd = 2, lty = 2)

  # Observed data points
  points(dat$Date[zoom_idx_obs], data$y[zoom_idx_obs], pch = 21, bg = "white")

  # Forecasted points
  points(future_dates[zoom_idx_forecast], X_median_forecast[zoom_idx_forecast], pch = 4, bg = "red", col = "red")

  # Legend
  legend("topleft",
         legend = c("Observed Median", "Forecast Median", "Observed Data", "95% CI (Observed)", "95% CI (Forecast)"),
         col = c("blue", "red", "black", NA, NA),
         lwd = c(2, 2, NA, NA, NA),
         lty = c(1, 2, NA, NA, NA),
         pch = c(NA, NA, 21, NA, NA),
         pt.bg = c(NA, NA, "white", NA, "red"),
         fill = c(NA, NA, NA, rgb(0, 0, 1, 0.2), rgb(1, 0, 0, 0.2)),
         border = c(NA, NA, NA, NA, NA),
         bty = "n",
         cex = 0.8)
}
zoomed_plot()
```

Let's start by partitioning out the initial condition uncertainty.

```{r}
## Initial settings
Nmc = 1000            ## Number of Monte Carlo draws
NT = length(dat$tick) ## Length of time (same as number of observed ticks)
time = 1:NT           ## Time vector for plotting
time_forecast = (NT + 1):(NT + 12) ## Forecast time points
ylim = c(0, max(dat$tick) * 1.1) ## Set y-range for plotting
trans = 0.8           ## Transparency for plot shading
N.cols <- c("black", "blue", "red", "green") ## Colors for plotting

## For forward simulation, let's use the average of the parameters from the posterior distribution
params <- as.matrix(params_df) 
param.mean <- apply(params, 2, mean) 


```

We want to write a function that plots just the CI around our known data
```{r}
plot.run <- function(){
# Base plot
plot(dat$Date, X_median, type = 'l', lwd = 2, col = "blue", ylim = c(-100, max(X_CI[2,]) * 1.1),
     ylab = "N", xlab = "Date")
# 95% Credible interval as blue ribbon
polygon(c(dat$Date, rev(dat$Date)),
        c(X_CI[1,], rev(X_CI[2,])),
        col = rgb(0, 0, 1, 0.2), border = NA)
# Add median line again on top of ribbon
lines(dat$Date, X_median, col = "blue", lwd = 2)
# Observed data points
points(dat$Date, data$y, pch = 21, bg = "white")
}
```

```{r}
plot.run()
```



I don't think we need this with Nick's param.mean

Trying to figure out how to write this function with our time steps
- I think the tau[t] should work in that aspect
- I am just concerned about the [,t-1] but I was trying to follow the format of the function that was in the TBL
- I also don't know if it makes sense to have generalize the number of ticks being predicted, it may only make sense to do that for the process error
```{r}
forecastN <- function(N_ic,K,r,sigma,tau_N_ic, n=n){
  N_store <- matrix(NA,n,NT)  ## storage
  Nprev <- N_ic           ## initialize
  for(t in 1:NT){
    N = pmax(0,((K*N[,t-1]) / (N[,t-1] + (K-N[,t-1]))*exp(-r*tau[t])) + E[,t]) #calculate # ticks
    tau_E[,t] = sigma*tau[t]  
    E[,t] = rnorm(0,1/tau_E[,t])
    
  }
  return(N)
}
```
