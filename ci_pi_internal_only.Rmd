---
title: "combined model"
author: "Talladega Bites"
date: "2025-04-02"
output: html_document
---

#Talladega Bites

```{r}
#remotes::install_github("eco4cast/neon4cast")
library(tidyverse)
library(neon4cast)
library(lubridate)
library(rMR)
library(arrow)
library(rjags)
require(ggplot2)
library(dplyr)
library(tidyr)
```

# Tick Data from Carrie
```{r}
dat <- read.csv("monthly_tick_temp.csv")
library(rjags)
```
The data is the tick and temperature data used in the other models. We only used data if we had the month/year available for both tick count and average monthly temperature. If there were two tick counts for a given month/year, we averaged them together so that we had an average for the month.

Formatting data:

```{r}
t = dat[1:58,1] #time
tick = round(dat[1:58,3]) #monthly average ticks
temp = dat[1:58,2] #monthly average temperature
data <- list(tick = tick, temp = temp, n = length(t))

data$temp <- round(data$temp) 
```


```{r}
plot(temp,tick)
```


# Quick Model Summary

For our combined internal and external factors model, we estimate mean tick count at time *i* (X[i]) using a carrying capacity model (density dependence as an internal factor) where **K** (carrying capacity) and **r** (growth rate) are dependent on **temperature** (our chosen external factor, for a start) and the state equation includes a term for **process error** (E). 

$$
X_i = r_i \cdot X_{i-1} \cdot \left(1 - \frac{X_{i-1}}{K_i} \right) + E_i
$$
$$
r_i = \beta_0 + \beta_1 \cdot \text{temp}
$$

$$
K_i = \exp(\beta_0 + \beta_1 \cdot \text{temp})
$$
$$
E_i \sim \text{Normal}(0, \tau)
$$
$$
\tau \sim \text{Gamma}(0.1, 0.1)
$$

To fit the model to historical data, we need months where we have both a tick count and a mean temperature, which is why you see me dropping rows missing one or the other in the code blocks above. Below, I define the model in JAGS-speak. Note that process error (E), r, and K are defined for each model loop. B0, B1 are the temperature effects on the rate variable, and B2, B3 are the temperature effects on the K, which shouldn't be changing, so they are not defined in each loop. We also have priors on the initial conditions at X[1], since at the first time point, we aren't sure what the ticks or temp at t-1 were.

r and K are linear relationships for now, but that may have to be adjusted. I have also wrapped K in an exponential function to avoid any negative values. I initially just put a max(0.001) term on it, but that introduced issues with the traceplots (the chains were getting hung at 0.001); using exp() gives the distribution smoother approach to 0 and allows the MCMC to keep running more smoothly.

-- look at correlation plots. Look for weird T shape (potato good, hugging is bad).
-- constrain parts of model. Set K as hard 300, can help 
-- run for 150 iters
-- can try running without temp dependence (temp seems to not be influencing much)



```{r}
# Convert 'month.year' to a Date object (assuming format is YYYY-MM)
dat$Date <- as.Date(paste0(dat$month.year, "-01"))

# Calculate time step differences in months
dat$TimeStep <- c(NA, diff(as.numeric(format(dat$Date, "%Y")) * 12 + as.numeric(format(dat$Date, "%m"))))

dat <- dat[1:58,]
dat$tick <- round(dat$tick)

# View result
head(dat)

```

```{r}
log_solution_model <- "
model{

  ## priors
  ## tau (time between steps) from the dataframe
  K <- 500
  r ~ dnorm(0,0.01)
  sigma ~ dgamma(0.1,0.1)
  
  N[1] ~ dnorm(N_ic, tau_N_ic)  # Latent state initial value
  N_ic ~ dnorm(0, 0.1)        # Prior for initial state
  tau_N_ic ~ dgamma(0.1, 0.1)   # Precision on initial state
  
  ## process model
    for(i in 2:Ni){
      tau_E[i] <- sigma*sqrt(2*tau[i])
      E[i] ~ dnorm(0, tau_E[i])
      N[i] <- ((K*N[i-1]) / (N[i-1] + (K-N[i-1]))*exp(-r*tau[i])) + E[i]
    }
  
  ## data model
    for(i in 1:Ni){
      y[i] ~ dpois(max(0.001, N[i]))  # Ensures positive values only
    }
}
"
```

```{r}
data <- list(y=dat$tick, Ni=length(dat$tick),      ## tick data
             tau=dat$TimeStep
             )

j.model   <- jags.model (file = textConnection(log_solution_model),
                             data = data,
                             n.chains = 5)
```

And here we have the actual model running step. You can see that I have a lot of iterations and a burn-in period to help with convergence (which we're still not achieving with most, if not all, of the variables).

```{r}
out_1 <- coda.samples(model = j.model,
                      variable.names = c("r", "y", "E", "N", "sigma"),
                      n.iter = 150000,
                      burnin = 10000)

```


```{r}
plot(out_1[, c("r", "E[5]", "E[10]")])
```

```{r}
plot(out_1[, c("sigma")])
```

```{r}
gelman.plot(out_1[, c("r", "sigma")])
```


```{r}
#gelman.plot(out_1[, c("y[5]", "y[10]", "y[15]")])

# the file won't knit if this line runs -- but here is the error i get:

#******* Error: *******
#Cannot compute Gelman & Rubin's diagnostic for any chain 
#segments for variables y[5] y[10] y[15] 
#This indicates convergence failure

```

```{r}
gelman.plot(out_1[, c("E[5]", "E[10]", "E[15]")])
```


None of these look like they're converging to me except for the process error terms. The y terms are so bad that the gelman function can't even plot them.


```{r}
gelman.diag(out_1[, c("r", "sigma")])
```

```{r}
#gelman.diag(out_1[, c("y[5]", "y[10]", "y[15]")])

# again, file won't knit with this line. Error below:
# Error in chol.default(W) : the leading minor of order 1 is not positive
```

```{r}
gelman.diag(out_1[, c("E[5]", "E[10]", "E[15]")])
```

Above, confirming what the traceplots and BGR plots (and my heart) told us, none of the BGR metrics are below 1.1, which tells us the variables have not converged. :( Except for our superstar E (process error) term! It's nice to have a positive control for what a converged variable might look like, if nothing else.

Let's look at some summary statistics:
```{r}
summary(out_1[, c("y[5]", "y[10]", "y[15]",
              "r",
              "E[5]", "E[10]", "E[15]")])
```

```{r}

combined_mcmc <- as.mcmc(do.call(rbind, out_1))

# convert to data frame
params_df <- as.data.frame(combined_mcmc)


params_subset <- params_df[, c("E[5]", "E[10]", "E[15]", "r", "sigma")]
head(params_subset)
```
```{r}
pairs(params_subset, pch = 1, cex = 0.3)
```

# Time Series

```{r}
# Flatten MCMC output
out_matrix <- as.matrix(out_1)

# Time vector
time <- 1:length(dat$tick)

# Extract latent state samples
X_samples <- out_matrix[, grep("^N\\[", colnames(out_matrix))]

# Compute posterior summaries
X_median <- apply(X_samples, 2, median)
X_CI <- apply(X_samples, 2, quantile, probs = c(0.025, 0.975))

```

```{r}

# Base plot
plot(dat$Date, X_median, type = 'l', lwd = 2, col = "blue", ylim = c(-100, max(X_CI[2,]) * 1.1),
     ylab = "N", xlab = "Date")

# 95% Credible interval as blue ribbon
polygon(c(dat$Date, rev(dat$Date)),
        c(X_CI[1,], rev(X_CI[2,])),
        col = rgb(0, 0, 1, 0.2), border = NA)

# Add median line again on top of ribbon
lines(dat$Date, X_median, col = "blue", lwd = 2)

# Observed data points
points(dat$Date, data$y, pch = 21, bg = "white")

legend("topright",
       legend = c("Median latent state", "Observed counts", "95% Credible Interval"),
       col = c("blue", "black", NA),
       lwd = c(2, NA, NA),
       pch = c(NA, 21, NA),
       pt.bg = c(NA, "white", NA),
       fill = c(NA, NA, rgb(0, 0, 1, 0.2)),  # Add fill for CI
       border = c(NA, NA, NA),              # No border for fill
       bty = "n",
       cex = 0.8)


```


# add forecast interval to JAGS model

```{r}
log_solution_forecast <- "
model{

  ## priors
  ## tau (time between steps) from the dataframe
  K <- 500
  r ~ dnorm(0,0.01)
  sigma ~ dgamma(0.1,0.1)
  
  N[1] ~ dnorm(N_ic, tau_N_ic)  # Latent state initial value
  N_ic ~ dnorm(0, 0.1)        # Prior for initial state
  tau_N_ic ~ dgamma(0.1, 0.1)   # Precision on initial state
  
  ## process model
    for(i in 2:Ni){
      tau_E[i] <- sigma*sqrt(2*tau[i])
      E[i] ~ dnorm(0, tau_E[i])
      N[i] <- ((K*N[i-1]) / (N[i-1] + (K-N[i-1]))*exp(-r*tau[i])) + E[i]
    }
  
  ## data model
    for(i in 1:Ni){
      y[i] ~ dpois(max(0.001, N[i]))  # Ensures positive values only
    }
}
"
```

```{r}
N_forecast <- 12
Ni_obs <- length(dat$tick)

# Extend y with 12 NAs
y_full <- c(dat$tick, rep(NA, N_forecast))

# Assume constant monthly step from last timestep
# If TimeStep is in months (e.g., 1, 2, 3...), this just extends linearly
last_tau <- tail(dat$TimeStep, 1)
tau_forecast <- rep(1, N_forecast)  # each new step is 1 month
tau_full <- c(dat$TimeStep, tau_forecast)

# Total time points
Ni <- length(y_full)

data_forecast <- list(
  y = y_full,
  tau = tau_full,
  Ni = Ni
)

```

```{r}
j.model <- jags.model(
  file = textConnection(log_solution_model),
  data = data_forecast,
  n.chains = 5
)


out_forecast <- coda.samples(
  model = j.model,
  variable.names = c("r", "y", "E", "N", "sigma"),
  n.iter = 5000,
  burnin = 1000
)


```

```{r}
plot(out_forecast[, c("r", "E[5]", "E[10]")])
```

```{r}
# Convert coda output to matrix
out_matrix <- as.matrix(out_forecast)

# Extract columns corresponding to N[...]
N_cols <- grep("^N\\[", colnames(out_matrix))
N_samples <- out_matrix[, N_cols]  # Each column is N[1], N[2], ..., N[Ni + 12]

# Confirm dimension
dim(N_samples)  # should be (n.iter * n.chains) rows Ã— (Ni + 12) columns
```

```{r}
# Compute posterior median and 95% CI for each time point
N_median <- apply(N_samples, 2, median)
N_CI <- apply(N_samples, 2, quantile, probs = c(0.025, 0.975))

```

```{r}
Ni_obs <- length(dat$Date)
Ni_total <- ncol(N_samples)
Ni_forecast <- Ni_total - Ni_obs

# Extend date vector 12 months forward
future_dates <- seq(from = max(dat$Date) + 1, by = "month", length.out = Ni_forecast)
all_dates <- c(dat$Date, future_dates)

# Split CI and median into observed + forecast parts
X_median_obs <- N_median[1:Ni_obs]
X_CI_obs <- N_CI[, 1:Ni_obs]

X_median_forecast <- N_median[(Ni_obs + 1):Ni_total]
X_CI_forecast <- N_CI[, (Ni_obs + 1):Ni_total]

```


```{r}
# Base plot
plot(all_dates, N_median, type = 'n', ylim = c(-100, max(N_CI[2,]) * 1.1),
     ylab = "N", xlab = "Date")

# 95% CI for observed
polygon(c(dat$Date, rev(dat$Date)),
        c(X_CI_obs[1,], rev(X_CI_obs[2,])),
        col = rgb(0, 0, 1, 0.2), border = NA)

# 95% CI for forecast
polygon(c(future_dates, rev(future_dates)),
        c(X_CI_forecast[1,], rev(X_CI_forecast[2,])),
        col = rgb(1, 0, 0, 0.2), border = NA)

# Median lines
lines(dat$Date, X_median_obs, col = "blue", lwd = 2)
lines(future_dates, X_median_forecast, col = "red", lwd = 2, lty = 2)

# Observed data points
points(dat$Date, data$y, pch = 21, bg = "white")

# Forecasted points
points(future_dates, X_median_forecast, pch = 4, bg = "red", col = "red")

# Legend
legend("topleft",
       legend = c("Observed Median", "Forecast Median", "Observed Data", "95% CI (Observed)", "95% CI (Forecast)"),
       col = c("blue", "red", "black", NA, NA),
       lwd = c(2, 2, NA, NA, NA),
       lty = c(1, 2, NA, NA, NA),
       pch = c(NA, NA, 21, NA, NA),
       pt.bg = c(NA, NA, "white", NA, "red"),
       fill = c(NA, NA, NA, rgb(0, 0, 1, 0.2), rgb(1, 0, 0, 0.2)),
       border = c(NA, NA, NA, NA, NA),
       bty = "n",
       cex = 0.8)

```



