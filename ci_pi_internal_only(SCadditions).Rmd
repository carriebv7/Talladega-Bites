---
title: "Final Ticks Forecasting Model"
author: "Talladega Bites"
date: "2025-04-02"
output: html_document
---

#Talladega Bites

```{r}
#remotes::install_github("eco4cast/neon4cast")
library(tidyverse)
library(neon4cast)
library(lubridate)
library(rMR)
library(arrow)
library(rjags)
require(ggplot2)
library(dplyr)
library(tidyr)
#install.packages("scales")
library(scales)
```

# Tick Data
We loaded the tick-count data set for Talledaga (monthly_tick_temp.csv), which was manually parsed from the raw data to extract monthly counts. The total observations are 58. 

```{r}
dat <- read.csv("monthly_tick_temp.csv")
dat <- dat[1:58, c("month.year", "tick")]# # drops unneeded columns and NA rows
```

```{r}
t = dat[1:58,1] #time

```


# Quick Model Summary 

Process Model:
We represent the average monthly tick abundance (N) with the solution to the logistic-growth differential equation plus process error. At each month i, the population evolves according to:
$$
\begin{aligned}
N_{\mathrm{ticks}}[i] 
  &= \frac{K\,N_{\mathrm{ticks}}[i-1]}
         {N_{\mathrm{ticks}}[i-1] + (K - N_{\mathrm{ticks}}[i-1])\,\exp^{-r\,\tau[i]}}
     + E[i], \\[15pt]
where \\[5pt]
r           &\sim \mathrm{Normal}(0,\,0.01)\\
K           &\sim \mathrm{Normal}(300,\,0.01)\\
E[i]        &\sim \mathrm{Normal}\bigl(0,t_{E}[i]\bigr)\\
t_{E}[i] &\leftarrow \sigma^{\tau[i]}\\
\sigma      &\sim \mathrm{Gamma}(0.1,\,0.1)
\end{aligned}
$$
N = estimated mean number of ticks per month
K = carrying capacity
r = intrinsic growth rate
Tau = time step

Here, Tau[i] is the elapsed time between observations, ensuring the model accommodates any irregular spacing. This formulation captures time-step scaling growth.
We chose an uninformed prior for r and sigma. K has a small precision, but has an informed mean of 300 to help with model convergence. Our t_{E}[i] term is multiplicative to take into account the time steps that are greater than one.

Data Model:
We chose a Poisson distribution for our data model since we expect positive tick counts.
$$
y[i]\sim pois(N_{\mathrm{ticks}})
$$

#Data formatting for JAGS input
We convert the month.year strings into proper Date objects (first of each month), compute the month‐to‐month intervals to scale our model, round the tick counts to integers for the Poisson model, and then bundle the key components—tick counts, number of observations, and time steps into a list for JAGS.
```{r}
# Convert 'month.year' to a Date object
dat$Date <- as.Date(paste0(dat$month.year, "-01"))

# Calculate time step differences in months
dat$TimeStep <- c(NA, diff(as.numeric(format(dat$Date, "%Y")) * 12 + as.numeric(format(dat$Date, "%m"))))


# Ensure integeger counts for Poisson likelihood
dat$tick <- round(dat$tick)
dat$tick <- round(dat$tick)
head(dat)
```

```{r}
# Prepare data list for JAGS
tick <- dat$tick
data <- list(tick = tick, n = length(t))
```


We then implement the above model in JAGS, specifying priors for K, r, Sigma, and the latent initial state, looping through the process‐model update for each time step, and linking to the data via a Poisson likelihood.
```{r}
log_solution_model <- "
model{

  ## priors
  ## tau (time between steps) from the dataframe
  K ~ dnorm(300,0.01)     # from our 'literature review'
  r ~ dnorm(0,0.01)       # uninformative prior on rate
  sigma ~ dgamma(0.1,0.1) ## sigma is E's precision for one month
  
  N[1] ~ dnorm(N_ic, tau_N_ic)  # Latent state initial value
  N_ic ~ dnorm(0, 0.1)        # Prior for initial state
  tau_N_ic ~ dgamma(0.1, 0.1)   # Precision on initial state
  
  ## process model
    for(i in 2:Ni){
      tau_E[i] <- sigma^tau[i]   # sigma is multiplicative for multiple months
      E[i] ~ dnorm(0, tau_E[i])
      N[i] <- max(0,((K*N[i-1]) / (N[i-1] + (K-N[i-1]))*exp(-r*tau[i])) + E[i])  # wrapped in max so E[i] doesn't drag N[i] <0
    }
  
  ## data model
    for(i in 1:Ni){
      y[i] ~ dpois(max(0.001, N[i]))  # Ensures positive values only
    }
}
"
```

#JAGS model initialization. 
Next, we bundle the processed data into a list matching the variables expected by the JAGS script, namely: the observed counts y, the number of time points Ni, and the month‐to‐month intervals Tau. We then compile the model defined in log_solution_model and initialize 5 MCMC chain.
```{r}
data <- list(y=dat$tick, 
             Ni=length(dat$tick), 
             tau=dat$TimeStep
             )

j.model   <- jags.model (file = textConnection(log_solution_model),
                             data = data,
                             n.chains = 5)
```

We're drawing from 150,000 iterations per chain to ensure convergence and discarding the first 10,000 as burn-in to reduce dependence on initial values. Convergence diagnostics are assessed in the following sections. 
```{r}
out_1 <- coda.samples(model = j.model,
                      variable.names = c("r", "E", "N", "K", "sigma"),
                      n.iter = 150000,
                      burnin = 10000)

```

#Convergence diagnostics and posterior distributions:
We visually inspected convergence by plotting trace and density plots for the key parameters (r, K, Sigma), three latent states (N[5], N[10], N[15]), and three process error draws (E[5], E[10], E[15]). Well‐mixed, “hairy caterpillar” trace plots with no apparent trends and smooth, unimodal density curves indicate that the chains have likely converged and explored the posterior distributions adequately.
```{r}
plot(out_1[, c("r", "K", "sigma")])
```

```{r}
plot(out_1[, c("N[5]", "N[10]", "N[15]")])
```

```{r}
plot(out_1[, c("E[5]", "E[10]", "E[15]")])
```

#Convergence confirmation via Gelman–Rubin diagnostics.
To investigate the convergence further, we examined the Gelman plots and diagnostics for the key parameters (r, K, sigma) and example latent‐state and process error draws (N[5], N[10], N[15], E[5], E[10], E[15]). In the Gelman plots, the median and 97.5%‐quantile shrink factors quickly fall to ~1 and remain stable throughout the chains, except for the parameter sigma. Aside from sigma and r, the tabulated PSRF point estimates and upper 97.5% credible limits are all ≤ 1.01, with a multivariate PSRF of exactly 1, indicating that all chains have mixed and converged successfully. The parameter r has a PSRF point estimate of 1.01 and CI of 1.02, which is still indicative of convergence. Sigma's values are 1.04 and 1.09, indicating that the parameter is not converging. In order to solve this problem, we could increase the number of iterations, or choose a more informed prior. 

# Diagnostics
```{r}
gelman.plot(out_1[, c("E[5]", "E[10]", "E[15]")])
```

```{r}
gelman.plot(out_1[, c("r", "K", "sigma")])
```




```{r}
gelman.diag(out_1[, c("r", "sigma", "K",
                      "N[5]", "N[10]", "N[15]",
                      "E[5]", "E[10]", "E[15]"
                      )])
```


#Posterior summary statistics:
A few key takeaways from the posterior summaries:
-Intrinsic growth rate (r) has a posterior mean of ~0.18, indicating moderate density-dependent growth.
-Carrying capacity (K) is tightly estimated around 300 ticks, closely matching our prior.
-Process error precision (sigma) is small (∼0.0018), implying modest month-to-month variability.

```{r}
summary(out_1[, c("r", "K", "sigma",
                  "N[5]", "N[10]", "N[15]",
                  "E[5]", "E[10]", "E[15]")])
```

```{r}

combined_mcmc <- as.mcmc(do.call(rbind, out_1))

# convert to data frame
params_df <- as.data.frame(combined_mcmc)

params_subset <- params_df[, c("r", "K", "sigma",
                               "N[5]", "N[10]", "N[15]",
                               "E[5]", "E[10]", "E[15]")]
head(params_subset)
```



#Notable observations from the pairs plot:
- The most noticable correlations in our parameters are between N and E of the same draw, (ie [5]). N is dependent on the amount of process error, E, which is why there is a strong correlation between both paramters.
-There is ittle to no correlation between the other parameters, which is to be expected.
```{r}
#pairs(params_subset, pch = 1, cex = 0.3)
```
(plots take a long time to run and often causes R to crash)


# Time Series
The figure below shows the posterior median estimate of monthly tick count (blue line) with its 95% credible interval (blue shading) compared to the observed tick counts (white-filled circles). The model closely matches the peaks and troughs in the data. The credible intervals widens slightly where variability is greatest, which is expected as we enter stretches of missing data (e.g., over winter months). The uncertainty accumulates and the ribbon grows accordingly.
```{r}
# Flatten MCMC output
out_matrix <- as.matrix(out_1)

# Time vector
time <- 1:length(dat$tick)

# Extract latent state samples
X_samples <- out_matrix[, grep("^N\\[", colnames(out_matrix))]

# Compute posterior summaries
X_median <- apply(X_samples, 2, median)
X_CI <- apply(X_samples, 2, quantile, probs = c(0.025, 0.975))

```

```{r}

# Base plot
plot(dat$Date, X_median, type = 'l', lwd = 2, col = "blue", ylim = c(-100, max(X_CI[2,]) * 1.1),
     ylab = "N", xlab = "Date")

# 95% Credible interval as blue ribbon
polygon(c(dat$Date, rev(dat$Date)),
        c(X_CI[1,], rev(X_CI[2,])),
        col = rgb(0, 0, 1, 0.2), border = NA)

# Add median line again on top of ribbon
lines(dat$Date, X_median, col = "blue", lwd = 2)

# Observed data points
points(dat$Date, data$y, pch = 21, bg = "white")

legend("topright",
       legend = c("Median latent state", "Observed counts", "95% Credible Interval"),
       col = c("blue", "black", NA),
       lwd = c(2, NA, NA),
       pch = c(NA, 21, NA),
       pt.bg = c(NA, "white", NA),
       fill = c(NA, NA, rgb(0, 0, 1, 0.2)),  # Add fill for CI
       border = c(NA, NA, NA),              # No border for fill
       bty = "n",
       cex = 0.8)


```


# Forecasting
We now extend the observed 58-month series to 12 months in the future and rerun the same JAGS model. The plot below shows the results. The blue line and shading show the posterior median and 95% CI for the historical period (through the last observed date). The dashed red line and red shading show the 12-month forecast median and its 95% CI. The forecast intervals widen substantially, reflecting notable uncertainty in our forecast.
```{r}
N_forecast <- 12
Ni_obs <- length(dat$tick)

# Extend y with 12 NAs
y_full <- c(dat$tick, rep(NA, N_forecast))

# Assume constant monthly step from last timestep
# If TimeStep is in months (e.g., 1, 2, 3...), this just extends linearly
last_tau <- tail(dat$TimeStep, 1)
tau_forecast <- rep(1, N_forecast)  # each new step is 1 month
tau_full <- c(dat$TimeStep, tau_forecast)

# Total time points
Ni <- length(y_full)

data_forecast <- list(
  y = y_full,
  tau = tau_full,
  Ni = Ni
)

```

```{r}
j.model <- jags.model(
  file = textConnection(log_solution_model),
  data = data_forecast,
  n.chains = 5
)


out_forecast <- coda.samples(
  model = j.model,
  variable.names = c("r", "E", "N", "K", "sigma"),
  n.iter = 150000,
  burnin = 10000
)

```


```{r}
# Convert coda output to matrix
out_matrix <- as.matrix(out_forecast)

# Extract columns corresponding to N[...]
N_cols <- grep("^N\\[", colnames(out_matrix))
N_samples <- out_matrix[, N_cols]  # Each column is N[1], N[2], ..., N[Ni + 12]

# Confirm dimension
dim(N_samples)  # should be (n.iter * n.chains) rows by (Ni + 12) columns
```

```{r}
# Compute posterior median and 95% CI for each time point
N_median <- apply(N_samples, 2, median)
N_CI <- apply(N_samples, 2, quantile, probs = c(0.025, 0.975))

```

```{r}
Ni_obs <- length(dat$Date)
Ni_total <- ncol(N_samples)
Ni_forecast <- Ni_total - Ni_obs

# Extend date vector 12 months forward
future_dates <- seq(from = max(dat$Date) + 1, by = "month", length.out = Ni_forecast)
all_dates <- c(dat$Date, future_dates)

# Split CI and median into observed + forecast parts
X_median_obs <- N_median[1:Ni_obs]
X_CI_obs <- N_CI[, 1:Ni_obs]

X_median_forecast <- N_median[(Ni_obs + 1):Ni_total]
X_CI_forecast <- N_CI[, (Ni_obs + 1):Ni_total]

# Simulate Poisson predictive draws for forecast
N_forecast_samples <- N_samples[, (Ni_obs + 1):Ni_total]  # latent forecasts

# Matrix of predictive draws, same shape as N_forecast_samples
Y_pred_samples <- matrix(rpois(length(N_forecast_samples),
                               lambda = pmax(0.001, N_forecast_samples)),
                         nrow = nrow(N_forecast_samples))


Y_PI_forecast <- apply(Y_pred_samples, 2, quantile, probs = c(0.025, 0.975))

```


```{r}
# Base plot
plot(all_dates, N_median, type = 'n', ylim = c(-50, max(N_CI[2,]) * 1.1),
     ylab = "N", xlab = "Date")

# 95% CI for observed
polygon(c(dat$Date, rev(dat$Date)),
        c(X_CI_obs[1,], rev(X_CI_obs[2,])),
        col = rgb(0, 0, 1, 0.2), border = NA)

# 95% CI for forecast
polygon(c(future_dates, rev(future_dates)),
        c(X_CI_forecast[1,], rev(X_CI_forecast[2,])),
        col = rgb(1, 0, 0, 0.2), border = NA)

# Median lines
lines(dat$Date, X_median_obs, col = "blue", lwd = 2)
lines(future_dates, X_median_forecast, col = "red", lwd = 2, lty = 2)

# Observed data points
points(dat$Date, data$y, pch = 21, bg = "white")

# Forecasted points
points(future_dates, X_median_forecast, pch = 4, bg = "red", col = "red")

# Legend
legend("topleft",
       legend = c("Observed Median", "Forecast Median", "Observed Data",
                  "95% CI (Observed)", "95% CI (Forecast)"),
       col = c("blue", "red", "black", NA, NA),
       lwd = c(2, 2, NA, NA, NA),
       lty = c(1, 2, NA, NA, NA),
       pch = c(NA, NA, 21, NA, NA),
       pt.bg = c(NA, NA, "white", NA, "red", NA),
       fill = c(NA, NA, NA,
                rgb(0, 0, 1, 0.2),
                rgb(1, 0, 0, 0.2),
                rgb(1, 0.7, 0, 0.2)),
       border = NA, bty = "n", cex = 0.8)

```
In a logistic growth model, we would expect to see an increase towards the carrying capacity; however, the forecast predicts a decrease in tick population. We believe that the decrease is due to large process error. 


# Uncertainty Partitioning
Now, we have to partition our uncertainty. The median line above is our deterministic prediction. We will zoom in on 2023 and onward in the plot to better see our uncertainty, and take out the confidence interval for the forecasting period from before. In short, this plot tells us how the model’s central tendency evolves: a quick post‐peak collapse followed by slow decay to a potential steady state of the system.

```{r}
dat$Date <- as.Date(paste0(dat$month.year, "-01"))
# Define cutoff date
cutoff_date <- as.Date("2023-01-01")

# Filter indices for zooming
zoom_idx_all <- which(all_dates >= cutoff_date)
zoom_idx_obs <- which(dat$Date >= cutoff_date)
zoom_idx_forecast <- which(future_dates >= cutoff_date)

zoomed_plot <- function() {
  # Base plot (zoomed)
  plot(all_dates[zoom_idx_all], N_median[zoom_idx_all], type = 'n',
       ylim = c(-50, max(N_CI[2, zoom_idx_all]) * 1.1),
       ylab = "N", xlab = "Date")
  
  # 95% CI for observed
  polygon(c(dat$Date[zoom_idx_obs], rev(dat$Date[zoom_idx_obs])),
          c(X_CI_obs[1, zoom_idx_obs], rev(X_CI_obs[2, zoom_idx_obs])),
          col = rgb(0, 0, 1, 0.2), border = NA)

  # Median lines
  lines(dat$Date[zoom_idx_obs], X_median_obs[zoom_idx_obs], col = "blue", lwd = 2)
  lines(future_dates[zoom_idx_forecast], X_median_forecast[zoom_idx_forecast], col = "purple", lwd = 3, lty = 1)

  # Observed data points
  points(dat$Date[zoom_idx_obs], data$y[zoom_idx_obs], pch = 21, bg = "white")

}
zoomed_plot()
```

## Helper Functions
Now we streamline our forecasting and plotting workflow with three helper functions. First, forecastN generates an ensemble of n trajectories over steps months. Second, calc_forecast_ci takes the resulting forecast matrix and returns the 2.5% and 97.5% quantiles and the medians, facilitating quick access to credible intervals and central estimates. Finally, draw_env helps us to efficiently draw the CI envelopes. Together, these functions let us efficiently simulate, summarize, and visualize forecast uncertainty in a few concise lines of code.

```{r}
set.seed(111)
##' @param IC    Vector of initial conditions (length = n)
##' @param r     Scalar intrinsic growth rate
##' @param K     Scalar carrying capacity
##' @param tau   Vector of time steps (length = steps)
##' @param n     Size of Monte Carlo ensemble
##' @param steps Number of forecast steps (default = 12)
forecastN <- function(IC, r, K, tau, n = 1000, steps = 12) {
  N <- matrix(NA, n, steps)       # Forecast matrix
  Nprev <- IC                     # Starting values (should be length n)
  
  for (t in 1:steps) {
    mu <- ((K * Nprev) / (Nprev + (K - Nprev))) * exp(-r * tau[t])
    N[, t] <- pmax(0, mu)         # Deterministic projection
    Nprev <- N[, t]               # Advance to next time step
  }
  
  return(N)
}


# helper to compute 95% CI and median
calc_forecast_ci <- function(fmat) {
  list(
    ci = apply(fmat, 2, quantile, probs = c(0.025, 0.975)),
    median = apply(fmat, 2, median)
  )
}

# helper to draw ci envelopes
draw_env <- function(ci, dates, col) {
  polygon(c(dates, rev(dates)),
          c(ci[1,], rev(ci[2,])),
          col = col, border = NA)
}

```



## Initial condition uncertainty
Let's start by partitioning out the initial condition uncertainty.To isolate the contribution of uncertainty in the final latent‐state estimate alone, we sampled 1,000 draws of N[58] (the last inferred month’s abundance) from the posterior, while holding the growth rate r and carrying capacity fixed at their posterior means and omitting process error. We then used our forecastN() function to propagate only these initial‐condition variations forward for 12 months. The resulting grey ribbon in the zoomed plot shows the 95% interval arising solely from uncertainty in the starting state, and the black line marks its median forecast under this scenario. This envelope quantifies how much of the total forecast spread is attributable purely to ambiguity in the most recent observed abundance.

```{r}
posterior <- as.matrix(out_forecast)

# Sample ICs from the posterior distribution of N[58] (last latent state)
IC <- sample(posterior[,"N[58]"], size = 1000, replace = TRUE)

# Fix other parameters using posterior mean or a draw
r <- mean(posterior[, "r"])
K <- mean(posterior[, "K"])
tau_forecast <- rep(1, 12)  # constant 1-month steps

# Forecast
N.I <- forecastN(IC = IC, r = r, K = K, tau = tau_forecast, n = 1000, steps = 12)

```

```{r}

N.I.ci <- apply(N.I, 2, quantile, c(0.025, 0.5, 0.975))

zoomed_plot()
ecoforecastR::ciEnvelope(future_dates, N.I.ci[1,], N.I.ci[3,], col = rgb(0.6, 0.6, 0.6, 0.4))
lines(future_dates, N.I.ci[2,], col = "black", lwd = 1)

```



# Initial Condition uncertainty + Parameter uncertainty
Now we sample 1,000 draws of N[58], r, and K from the posterior, still omitting process error. We then forecast each draw deterministic for 12 months using our forecastN() function. The red envelope (N.IP) captures the combined uncertainty from both the initial state and the parameters. The grey ribbon shows the initial‐condition‐only uncertainty from the previous step. Visually, the red envelope is substantially wider than the grey one, indicating that parameter uncertainty contributes more to total forecast variance than initial‐condition ambiguity. This tells us that, although knowing the exact current tick count matters, refining our estimates of the growth rate and carrying capacity would yield the largest reduction in near‐term forecast error.

```{r}
n.mc <- 1000
steps <- 12
tau_forecast <- rep(1, steps)

# Sample ensemble from posterior
rows <- sample(1:nrow(posterior), n.mc, replace = TRUE)

ICs <- posterior[rows, "N[58]"]
rs  <- posterior[rows, "r"]
Ks  <- posterior[rows, "K"]

# Forecast with varying IC + param, no process noise
N.IP <- forecastN(IC = ICs, r = rs, K = Ks, tau = tau_forecast, n = n.mc, steps = steps)

# Summarize
N.IP.ci <- apply(N.IP, 2, quantile, probs = c(0.025, 0.5, 0.975))
```


```{r}
zoomed_plot()

# Red envelope for N.IP
ecoforecastR::ciEnvelope(future_dates, N.IP.ci[1,], N.IP.ci[3,], col = rgb(1, 0, 0, 0.4))  # red with transparency
lines(future_dates, N.IP.ci[2,], lwd = 2, col = "red")

# envelope for N.I (initial condition only)
ecoforecastR::ciEnvelope(future_dates,N.I.ci[1,],N.I.ci[3,],col=rgb(0.6, 0.6, 0.6, 0.4))
lines(future_dates,N.I.ci[2,],lwd=0.5)

```


*Note*: we do not have any environmental drivers in our model, so calculating N.IPD will not add any additional uncertainty

# Layering in process noise (N.IPDE)
Next we extend our uncertainty partition by including the process error term. We redefine forecastN() to draw each monthly abundance from a normal distribution with the mean set as the result from our equation mu and the multiplicative standard deviation used in our original model. Sampling 1,000 draws of N[58], parameters(r, K), and process error precision (σ), we propagate each trajectory forward with process error. The resulting dark‐blue envelope (N.IPDE) shows the 95% CIs when both initial‐condition, parameter, and process error uncertainties are included. Overlaid in lighter red and gray are the previous two envelopes (N.IP and N.I), which show how each additional source of uncertainty incrementally widens our forecast intervals.

```{r}
# redefine function to include process noise term
forecastN <- function(IC, r, K, sigma, tau, n = 1000, steps = 12) {
  N <- matrix(NA, n, steps)
  Nprev <- IC
  
  for (t in 1:steps) {
    mu <- ((K * Nprev) / (Nprev + (K - Nprev))) * exp(-r * tau[t])
    N[, t] <- pmax(0, rnorm(n, mean = mu, sd = sigma^tau[t]))  # Add process error
    Nprev <- N[, t]
  }
  
  return(N)
}


```

```{r}
n.mc <- 1000
steps <- 12
tau_forecast <- rep(1, steps)

rows <- sample(1:nrow(posterior), n.mc, replace = TRUE)

ICs    <- posterior[rows, "N[58]"]
rs     <- posterior[rows, "r"]
Ks     <- posterior[rows, "K"]
sigmas <- posterior[rows, "sigma"]

# Now include process noise
N.IPDE <- forecastN(IC = ICs, r = rs, K = Ks, sigma = sigmas,
                    tau = tau_forecast, n = n.mc, steps = steps)

# Summarize
N.IPDE.ci <- apply(N.IPDE, 2, quantile, probs = c(0.025, 0.5, 0.975))

```

```{r}
zoomed_plot()

# Dark blue envelope for N.IPDE
ecoforecastR::ciEnvelope(future_dates, N.IPDE.ci[1,], N.IPDE.ci[3,],
                         col = rgb(0, 0, 0.6, 0.4))  # dark blue with transparency

# Median line in matching blue
lines(future_dates, N.IPDE.ci[2,], lwd = 2, col = "darkblue")

# Optionally overlay earlier layers
ecoforecastR::ciEnvelope(future_dates, N.IP.ci[1,], N.IP.ci[3,], col = rgb(1, 0, 0, 0.3))  # red
lines(future_dates, N.IP.ci[2,], lwd = 1, col = "red")

ecoforecastR::ciEnvelope(future_dates, N.I.ci[1,], N.I.ci[3,], col = rgb(0.6, 0.6, 0.6, 0.3))  # gray
lines(future_dates, N.I.ci[2,], lwd = 1, col = "gray40")

```

# Layer in random effect
Since we don't have any random effect in the model, we're going to add some noise to our sigma term to simulate it. We draw 1,000 posterior samples of N[58], growth rate, carrying capacity, and process error precision, and then multiply each noise precision by a random factor that is uniformly sampled from 0.8-1.2. using our forecastN() we generate a new forecast matrix and compute its 95% CIs. The resulting orange ribbon (N.IPDEA) sits almost entirely within the dark‐blue process‐noise envelope (N.IPDE), indicating that this extra random‐effect variance contributes only marginally to total forecast uncertainty (much less so than the core process‐noise and parameter components).

```{r}
n.mc <- 1000
steps <- 12
tau_forecast <- rep(1, steps)

rows <- sample(1:nrow(posterior), n.mc, replace = TRUE)

ICs    <- posterior[rows, "N[58]"]
rs     <- posterior[rows, "r"]
Ks     <- posterior[rows, "K"]
sigmas <- posterior[rows, "sigma"]

# Simulate added random-effect variance (+/- 20% around sigma)
rand_sigma <- sigmas * runif(n.mc, 0.8, 1.2)

# Forecast
N.IPDEA <- forecastN(IC = ICs, r = rs, K = Ks, sigma = rand_sigma,
                     tau = tau_forecast, n = n.mc, steps = steps)

# Summarize
N.IPDEA.ci <- apply(N.IPDEA, 2, quantile, probs = c(0.025, 0.5, 0.975))

```

```{r}
zoomed_plot()

# Orange envelope for N.IPDEA
ecoforecastR::ciEnvelope(future_dates, N.IPDEA.ci[1,], N.IPDEA.ci[3,],
                         col = rgb(1, 0.5, 0, 0.4))  # semi-transparent orange
lines(future_dates, N.IPDEA.ci[2,], lwd = 2, col = "orange3")

# Layer earlier envelopes
ecoforecastR::ciEnvelope(future_dates, N.IPDE.ci[1,], N.IPDE.ci[3,], col = rgb(0, 0, 0.6, 0.3))  # dark blue
lines(future_dates, N.IPDE.ci[2,], lwd = 1, col = "darkblue")

ecoforecastR::ciEnvelope(future_dates, N.IP.ci[1,], N.IP.ci[3,], col = rgb(1, 0, 0, 0.3))        # red
lines(future_dates, N.IP.ci[2,], lwd = 1, col = "red")

ecoforecastR::ciEnvelope(future_dates, N.I.ci[1,], N.I.ci[3,], col = rgb(0.6, 0.6, 0.6, 0.3))    # gray
lines(future_dates, N.I.ci[2,], lwd = 1, col = "gray40")

```

# Stacked Error Partitions
Lastly, we need to quantify and visualize how each source of uncertainty contributes to total forecast variance over the 12-month forecast. First, we compute the raw variances of the four forecast ensembles (initial‐condition only (N.I), initial + parameter (N.IP), +process noise (N.IPDE), and +random effect (N.IPDEA)) for each forecast month. We then normalize each column by its total variance and stack the resulting proportions in an area plot.
The black layer (initial‐condition) remains present throughout but decays from its peak contribution, falling close to zero by around April.
The blue layer (parameter uncertainty) grows quickly and dominates the forecast variance for most of the 12 months.
The red layer (process noise) contributes noticeably only in the first few months before thinning out almost entirely.
The green layer (random effect) starts small relative to the process noise and then slowly increases its share over time.
Overall, narrowing our uncertainty in the growth rate and carrying capacity (the blue band) would yield the biggest gains in forecast precision.

```{r}
### calculation of variances
varI     <- apply(N.I,2,var)
varIP    <- apply(N.IP,2,var)
varIPDE   <- apply(N.IPDE,2,var)
varIPDEA <- apply(N.IPDEA,2,var)
varMat   <- rbind(varI,varIP,varIPDE,varIPDEA)

## out-of-sample stacked area plot
V.pred.rel <- apply(varMat,2,function(x) {x/max(x)})
plot(future_dates,V.pred.rel[1,],ylim=c(0,1),type='n',main="Relative Variance: Out-of-Sample",ylab="Proportion of Variance",xlab="time")
ecoforecastR::ciEnvelope(future_dates,rep(0,ncol(V.pred.rel)),V.pred.rel[1,],col="black")
ecoforecastR::ciEnvelope(future_dates,V.pred.rel[1,],V.pred.rel[2,],col="blue")
ecoforecastR::ciEnvelope(future_dates,V.pred.rel[2,],V.pred.rel[3,],col="red")
ecoforecastR::ciEnvelope(future_dates,V.pred.rel[3,],V.pred.rel[4,],col="green")
legend("topright",legend=c("RandomEffect","Process","Parameter","InitCond"),col=c("green", "red", "blue", "black"),lty=1,lwd=5)
```


#Conlcusion
In conclusion, our time series model successfully captured the dynamics of tick counts at Talladega and provided a framework for forecasting and uncertainty analysis. The process model reproduced historical peaks and troughs. Posterior diagnostics confirmed that key parameters (r, K) and latent states were well-identified and converged reliably in JAGS.

Our 12-month ensemble forecast ultimately proved uninformative. The 95% CIs expanded so rapidly that they encompassed nearly all plausible tick-count values, yielding no clear directional trend. However, our uncertainty-partitioning analysis revealed that the forecast spread is substantially driven by parameter uncertainty in the growth rate and carrying capacity, and less so by process error, initial-condition uncertainty, and simulated random effects.


Moving forward, we would like to incorporate environmental drivers (e.g., temperature, humidity) since tick populations are highly dependent on these factors. However, it is important to note that adding complexity to our model would likely decrease process error, but increase parameter error, which is already our highest source of error. In the future, we would also like to expand to more sites and include additional or new observations as they become available could further refine our forecast and reduce uncertainty. 